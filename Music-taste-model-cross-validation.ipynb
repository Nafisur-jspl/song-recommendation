{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[2]) created by __init__ at <ipython-input-1-15ea84b661dc>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-15ea84b661dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRankingMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sommy/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/sommy/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 272\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[2]) created by __init__ at <ipython-input-1-15ea84b661dc>:7 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taste_file = os.path.join('.','data','subset_train_taste_profile.csv')\n",
    "taste_raw_data = sc.textFile(taste_file)\n",
    "taste_raw_data_header = taste_raw_data.take(1)[0]\n",
    "# Remove first row as header, split each row into token\n",
    "taste_data = taste_raw_data.filter(lambda line: line!=taste_raw_data_header)\\\n",
    "            .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),int(tokens[2]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = os.path.join('.','data','subset_test_taste_profile.csv')\n",
    "test_raw_data = sc.textFile(test_file)\n",
    "test_raw_data_header = test_raw_data.take(1)[0]\n",
    "# Remove first row as header, split each row into token\n",
    "test_data = test_raw_data.filter(lambda line: line!=test_raw_data_header)\\\n",
    "            .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),int(tokens[2]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2538996, 2552008)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taste_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model selection (ALS parameter tuning)\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5L\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "rank = 8\n",
    "errors = 0\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "model = ALS.trainImplicit(taste_data, rank, seed=seed, iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model with Mean average precision\n",
    "# Prepare prediction (recommendation)\n",
    "k = 500\n",
    "\n",
    "\n",
    "userRecommended = model.recommendProductsForUsers(k)\n",
    "user_reco = userRecommended.map(lambda x: (x[0], [r.product for r in x[1]]))\n",
    "\n",
    "# Labels data\n",
    "#user_songs = taste_data.map(lambda x: (x[0], x[1])).groupByKey().mapValues(list)\n",
    "user_songs = test_data.map(lambda x: (x[0], x[1])).groupByKey().mapValues(list)\n",
    "\n",
    "predictionAndLabels = user_reco.join(user_songs)\n",
    "test_predictionAndLabels = predictionAndLabels.map(lambda x: x[1])\n",
    "\n",
    "metrics = RankingMetrics(test_predictionAndLabels)\n",
    "print metrics.meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.236833220292"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.236833220292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.1 10\n",
      "8 0.1 20\n",
      "8 1.0 10\n",
      "8 1.0 20\n",
      "8 10.0 10\n",
      "8 10.0 20\n",
      "12 0.1 10\n",
      "12 0.1 20\n",
      "12 1.0 10\n",
      "12 1.0 20\n",
      "12 10.0 10\n",
      "12 10.0 20\n"
     ]
    }
   ],
   "source": [
    "ranks = [8, 12, 20, 40]\n",
    "regularization_parameters  = [0.1, 1.0, 10.0]\n",
    "iterations = [10, 20]\n",
    "\n",
    "for rank, regularization_param, numIter in itertools.product(ranks, regularization_parameters, iterations):\n",
    "    print rank, regularization_param, numIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ranks = [8, 12, 20, 40]\n",
    "#regularization_parameters  = [0.1, 1.0, 10.0]\n",
    "#iterations = [10, 20]\n",
    "result_list = []\n",
    "ranks = [8, 12]\n",
    "regularization_parameters  = [0.1]\n",
    "iterations = [10]\n",
    "k = 500\n",
    "\n",
    "for rank, regularization_param, numIter in itertools.product(ranks, regularization_parameters, iterations):\n",
    "    model = ALS.trainImplicit(taste_data, rank, iterations=numIter, lambda_=regularization_param)\n",
    "    userRecommended = model.recommendProductsForUsers(k)\n",
    "    user_reco = userRecommended.map(lambda x: (x[0], [r.product for r in x[1]]))\n",
    "\n",
    "    # Labels data\n",
    "    user_songs = test_data.map(lambda x: (x[0], x[1])).groupByKey().mapValues(list)\n",
    "\n",
    "    predictionAndLabels = user_reco.join(user_songs)\n",
    "    test_predictionAndLabels = predictionAndLabels.map(lambda x: x[1])\n",
    "\n",
    "    metrics = RankingMetrics(test_predictionAndLabels)\n",
    "    result_list.append((rank, regularization_param, numIter, metrics.meanAveragePrecision, metrics.precisionAt(k), metrics.ndcgAt(k)))\n",
    "    #print \"Rank: \" + str(rank)\n",
    "    #print \"Lambda: \" + str(regularization_param)\n",
    "    #print \"Iteration: \" + str(numIter)\n",
    "    #print \"Mean Average Precision: \" + str(metrics.meanAveragePrecision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result_list, columns=['rank','lambda','iteration','map','precision_k','ndcg_k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.1, 10, 0.0008288677125334717, 0.00515624073525052, 0.02057194518240263),\n",
       " (12,\n",
       "  0.1,\n",
       "  10,\n",
       "  0.0010158785493724215,\n",
       "  0.00558790394307738,\n",
       "  0.02325952648639946)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.to_csv('./data/cross_val_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./data/test2.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
